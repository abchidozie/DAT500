{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Overwrite Pyspark driver\n",
    "os.environ['PYSPARK_PYTHON'] = \"./environment/bin/python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from langdetect import detect\n",
    "import pyspark as ps\n",
    "import datetime\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import types as t\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "import re\n",
    "# from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"  \n",
    "def cleanText(text):\n",
    "    return re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
    "# Convert function to UDF\n",
    "cleanTextDF = f.udf(lambda z: cleanText(z))\n",
    "\n",
    "# Helper functions\n",
    "def isEnglish(text):\n",
    "    try:\n",
    "        if detect(text) != 'en':\n",
    "            return \"No\"\n",
    "        return \"Yes\"\n",
    "    except:\n",
    "        return \"No\"\n",
    "    return \"No\"\n",
    "# Convert function to UDF\n",
    "isEnglishUDF = f.udf(lambda z: isEnglish(z))\n",
    "\n",
    "# def summarizeFunction(text):\n",
    "#     summ = summarizer(text, min_length = round(0.1 * len(text.split(' '))), max_length = round(0.2 * len(text.split(' '))), do_sample=False)\n",
    "#     return summ\n",
    "\n",
    "# summarizeFunctionUDF = f.udf(lambda z: summarizeFunction(z),StringType())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2022-04-29 10:33:48,796 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2022-04-29 10:33:51,703 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"Redit Summarization App\")\\\n",
    "#     spark.archives\", \n",
    "#     .master(\"yarn\")\\\n",
    "#     .config(\"spark.executor.memoryOverhead\",\"2048\")\\\n",
    "#     .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "#     .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\\\n",
    "#     .getOrCreate()\n",
    "spark = SparkSession.builder.master(\"yarn\").appName(\"Reddit Summarization App\")\\\n",
    ".config(\"spark.yarn.dist.archives\",\"pyspark_venv.tar.gz#environment\")\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://namenode:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Reddit Summarization App</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7faf2f852a90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get spark configurations\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"hdfs://namenode:9000/dis_materials/data_reddit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "df2 = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('created_utc', 'string'),\n",
       " ('ups', 'string'),\n",
       " ('subreddit_id', 'string'),\n",
       " ('link_id', 'string'),\n",
       " ('name', 'string'),\n",
       " ('score_hidden', 'string'),\n",
       " ('author_flair_css_class', 'string'),\n",
       " ('author_flair_text', 'string'),\n",
       " ('subreddit', 'string'),\n",
       " ('id', 'string'),\n",
       " ('removal_reason', 'string'),\n",
       " ('gilded', 'string'),\n",
       " ('downs', 'string'),\n",
       " ('archived', 'string'),\n",
       " ('author', 'string'),\n",
       " ('score', 'string'),\n",
       " ('retrieved_on', 'string'),\n",
       " ('body', 'string'),\n",
       " ('distinguished', 'string'),\n",
       " ('edited', 'string'),\n",
       " ('controversiality', 'string'),\n",
       " ('parent_id', 'string')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2= df2.withColumn('created_utc', f.from_unixtime('created_utc').cast(t.DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.withColumn(\"ups\", df2[\"ups\"].cast(t.IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop null values\n",
    "df2 = df2.na.drop(subset=[\"subreddit\",\"subreddit_id\",\"body\",\"created_utc\",\"ups\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Remove comments belonging to moderators\n",
    "\"\"\"\n",
    "df2 = df2.filter((df2.distinguished != \"moderator\")|(df2.body!=\"[deleted]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use CleantTextDf to clean body column\n",
    "\"\"\"\n",
    "df2 = df2.withColumn(\"clean_body\",cleanTextDF(f.col(\"body\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.drop(\"name\",\"author_flair_css_class\",\"author_flair_text\",\"score_hidden\",\"id\",\"distinguished\",\"body\",\"removal_reason\",\"downs\",\"archived\",\"gilded\",\"retrieved_on\",\"edited\",\"controversiality\",\"author\",\"score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.where(f.length(f.col(\"parent_id\")) <= 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+------------+---------+---------------+----------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|created_utc|ups|subreddit_id|link_id  |subreddit      |parent_id |clean_body                                                                                                                                                                                                                                                                  |\n",
      "+-----------+---+------------+---------+---------------+----------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2015-05-01 |26 |t5_2rfxx    |t3_34ggke|leagueoflegends|t1_cqumqlm|and in the final match point game the real faker will 2v1 mid against faker 2 0 and easyhoon                                                                                                                                                                                |\n",
      "|2015-05-01 |2  |t5_2yyap    |t3_33gx0v|youdontsurf    |t1_cql0fqz|fuck you beeotch                                                                                                                                                                                                                                                            |\n",
      "|2015-05-01 |1  |t5_31xwr    |t3_34g6na|fatestaynight  |t1_cquesup|sorry there was only space for the most important crimes                                                                                                                                                                                                                    |\n",
      "|2015-05-01 |21 |t5_2qiel    |t3_34hhhx|hockey         |t1_cquqtqp|scarey perry                                                                                                                                                                                                                                                                |\n",
      "|2015-05-01 |1  |t5_2rrlp    |t3_34fy3a|PS4            |t1_cquklue|i know some reviewers had little or no working online early on                                                                                                                                                                                                              |\n",
      "|2015-05-01 |1  |t5_2qh1i    |t3_34dxlr|AskReddit      |t1_cqu83cv|i don t know how strange will it sound to you but here in my area no one brews beer or wine we brew a local version of whiskey but the process is not the same i have never seen the listed ingredients at the store or even available online i am talking for local vendors|\n",
      "|2015-05-01 |1  |t5_2yzcz    |t3_339nzl|ASFL           |t1_cqj2o1y|e looks like a massive fucking target is what it looks like lol god its going to be one hell of a pain in the ass for me to write procedures to defend that thing                                                                                                           |\n",
      "|2015-05-01 |1  |t5_2qh22    |t3_34gfze|anime          |t1_cquedhz|sounds like what somebody from killing floor would say if you would replace cash with dosh i have no idea what anime it would be from                                                                                                                                       |\n",
      "|2015-05-01 |2  |t5_2qh1q    |t3_34h7su|india          |t1_cquqz86|linked it some financial terms and shit in it which i did not understand let me know if you do                                                                                                                                                                              |\n",
      "|2015-05-01 |1  |t5_2qh2a    |t3_34fv2y|photography    |t3_34fv2y |keep them the you can do better changes as in jpeg sometimes you maybe like to modify a image                                                                                                                                                                               |\n",
      "|2015-05-01 |2  |t5_2qh1u    |t3_34goss|Music          |t1_cquk3a0|try this                                                                                                                                                                                                                                                                    |\n",
      "|2015-05-01 |2  |t5_2qjfk    |t3_34hhvv|stocks         |t1_cqur0qv|any reasons for enoc                                                                                                                                                                                                                                                        |\n",
      "|2015-05-01 |0  |t5_2qq91    |t3_34h3mn|Reprap         |t3_34h3mn |in related news when i first was grabbing the small baggy of delicious gummy bears to remove it from the bag i thought it was a condom                                                                                                                                      |\n",
      "|2015-05-01 |2  |t5_2w7et    |t3_34ey1p|FloridaMan     |t1_cqudl82|florida men don t have feelings the correct response to knowing you are living next to another florida man is yikes                                                                                                                                                         |\n",
      "|2015-05-01 |7  |t5_2rsy7    |t3_34grl5|Cityofheroes   |t1_cquq0pt|it really was well done it had just the right amount of customization and it was all intuitive                                                                                                                                                                              |\n",
      "|2015-05-01 |5  |t5_2qo4s    |t3_34hhx0|nba            |t1_cquqoke|probably a combo of both                                                                                                                                                                                                                                                    |\n",
      "|2015-05-01 |3  |t5_2rfak    |t3_34eyk6|lightingdesign |t3_34eyk6 |200 mac 101s i wish i had that type of money                                                                                                                                                                                                                                |\n",
      "|2015-05-01 |1  |t5_2qo4s    |t3_34hfcb|nba            |t1_cquptye|intelligent fans who would do anything to help their teams                                                                                                                                                                                                                  |\n",
      "|2015-05-01 |1  |t5_2rd8q    |t3_34gysg|Korean         |t3_34gysg |this might be a lot to cover but i listened to the ttmik podcasts as well as looked at the pdf s for them but these are still very confusing to me                                                                                                                          |\n",
      "|2015-05-01 |1  |t5_2s3j5    |t3_34bwzx|teslamotors    |t1_cquozg2|i d love to get in but it seems silly to just buy something like 25 shares if you can even do that wish i d got in years ago                                                                                                                                                |\n",
      "+-----------+---+------------+---------+---------------+----------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['created_utc',\n",
       " 'ups',\n",
       " 'subreddit_id',\n",
       " 'link_id',\n",
       " 'subreddit',\n",
       " 'parent_id',\n",
       " 'clean_body']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2.write.option(\"header\",\"true\").csv(\"hdfs://namenode:9000/cleaned_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for spark runner\n",
      "Looking for hadoop binary in /usr/local/hadoop/bin...\n",
      "Found hadoop binary: /usr/local/hadoop/bin/hadoop\n",
      "Looking for spark-submit binary in /usr/local/spark/bin...\n",
      "Found spark-submit binary: /usr/local/spark/bin/spark-submit\n",
      "Running step 1 of 1\n",
      "Creating temp directory /tmp/mapred.ubuntu.20220429.124659.723650\n",
      "  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "  Running Spark version 3.2.0\n",
      "  ==============================================================\n",
      "  No custom resources configured for spark.driver.\n",
      "  ==============================================================\n",
      "  Submitted application: harness.py\n",
      "  Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "  Limiting resource is cpu\n",
      "  Added ResourceProfile id: 0\n",
      "  Changing view acls to: ubuntu\n",
      "  Changing modify acls to: ubuntu\n",
      "  Changing view acls groups to: \n",
      "  Changing modify acls groups to: \n",
      "  SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ubuntu); groups with view permissions: Set(); users  with modify permissions: Set(ubuntu); groups with modify permissions: Set()\n",
      "  Successfully started service 'sparkDriver' on port 33265.\n",
      "  Registering MapOutputTracker\n",
      "  Registering BlockManagerMaster\n",
      "  Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "  BlockManagerMasterEndpoint up\n",
      "  Registering BlockManagerMasterHeartbeat\n",
      "  Created local directory at /tmp/blockmgr-fd83cfea-f827-4b59-8285-49b6899c9749\n",
      "  MemoryStore started with capacity 366.3 MiB\n",
      "  Registering OutputCommitCoordinator\n",
      "  Logging initialized @4082ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "  jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 1.8.0_312-8u312-b07-0ubuntu1~20.04-b07\n",
      "  Started @4227ms\n",
      "  Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "  Started ServerConnector@14908828{HTTP/1.1, (http/1.1)}{0.0.0.0:4041}\n",
      "  Successfully started service 'SparkUI' on port 4041.\n",
      "  Started o.s.j.s.ServletContextHandler@5b1a7151{/jobs,null,AVAILABLE,@Spark}\n",
      "  Started o.s.j.s.ServletContextHandler@53533865{/jobs/json,null,AVAILABLE,@Spark}\n",
      "  Started o.s.j.s.ServletContextHandler@21de00b9{/jobs/job,null,AVAILABLE,@Spark}\n",
      "  Started o.s.j.s.ServletContextHandler@6ae024cd{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "  Started o.s.j.s.ServletContextHandler@74742571{/stages,null,AVAILABLE,@Spark}\n",
      "  Started o.s.j.s.ServletContextHandler@75df7659{/stages/json,null,AVAILABLE,@Spark}\n",
      "  Started o.s.j.s.ServletContextHandler@75881666{/stages/stage,null,AVAILABLE,@Spark}\n",
      "  Started o.s.j.s.ServletContextHandler@70d63d54{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "  Started o.s.j.s.ServletContextHandler@60c6d506{/stages/pool,null,AVAILABLE,@Spark}\n",
      "  Started o.s.j.s.ServletContextHandler@2522a9b3{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "  Started o.s.j.s.ServletContextHandler@3e0f621e{/storage,null,AVAILABLE,@Spark}\n",
      "  Started o.s.j.s.ServletContextHandler@12b4dbfd{/storage/json,null,AVAILABLE,@Spark}\n",
      "  Started o.s.j.s.ServletContextHandler@a9e0de{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "  Started o.s.j.s.ServletContextHandler@4a6a44e0{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "  Started o.s.j.s.ServletContextHandler@5b883907{/environment,null,AVAILABLE,@Spark}\n",
      "  Started o.s.j.s.ServletContextHandler@15215347{/environment/json,null,AVAILABLE,@Spark}\n",
      "  Started o.s.j.s.ServletContextHandler@5fe176b2{/executors,null,AVAILABLE,@Spark}\n",
      "  Started o.s.j.s.ServletContextHandler@6fc4ae10{/executors/json,null,AVAILABLE,@Spark}\n",
      "  Started o.s.j.s.ServletContextHandler@271b6b7b{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "  Started o.s.j.s.ServletContextHandler@1ef76e67{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "  Started o.s.j.s.ServletContextHandler@4fc71e65{/static,null,AVAILABLE,@Spark}\n",
      "  Started o.s.j.s.ServletContextHandler@74f0c825{/,null,AVAILABLE,@Spark}\n",
      "  Started o.s.j.s.ServletContextHandler@10b2246a{/api,null,AVAILABLE,@Spark}\n",
      "  Started o.s.j.s.ServletContextHandler@b062221{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "  Started o.s.j.s.ServletContextHandler@49868739{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "  Bound SparkUI to 0.0.0.0, and started at http://namenode:4041\n",
      "  Added file file:///tmp/mapred.ubuntu.20220429.124659.723650/mrjob.zip at file:///tmp/mapred.ubuntu.20220429.124659.723650/mrjob.zip with timestamp 1651236425592\n",
      "  Copying /tmp/mapred.ubuntu.20220429.124659.723650/mrjob.zip to /tmp/spark-838102a8-6629-4626-aa81-c42e2fb6cefd/userFiles-2d38ecc2-dd68-4111-8bf5-109de1d8b0c6/mrjob.zip\n",
      "  Added file file:///tmp/mapred.ubuntu.20220429.124659.723650/script.zip at file:///tmp/mapred.ubuntu.20220429.124659.723650/script.zip with timestamp 1651236425592\n",
      "  Copying /tmp/mapred.ubuntu.20220429.124659.723650/script.zip to /tmp/spark-838102a8-6629-4626-aa81-c42e2fb6cefd/userFiles-2d38ecc2-dd68-4111-8bf5-109de1d8b0c6/script.zip\n",
      "  Starting executor ID driver on host namenode\n",
      "  Fetching file:///tmp/mapred.ubuntu.20220429.124659.723650/script.zip with timestamp 1651236425592\n",
      "  /tmp/mapred.ubuntu.20220429.124659.723650/script.zip has been previously copied to /tmp/spark-838102a8-6629-4626-aa81-c42e2fb6cefd/userFiles-2d38ecc2-dd68-4111-8bf5-109de1d8b0c6/script.zip\n",
      "  Fetching file:///tmp/mapred.ubuntu.20220429.124659.723650/mrjob.zip with timestamp 1651236425592\n",
      "  /tmp/mapred.ubuntu.20220429.124659.723650/mrjob.zip has been previously copied to /tmp/spark-838102a8-6629-4626-aa81-c42e2fb6cefd/userFiles-2d38ecc2-dd68-4111-8bf5-109de1d8b0c6/mrjob.zip\n",
      "  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35321.\n",
      "  Server created on namenode:35321\n",
      "  Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "  Registering BlockManager BlockManagerId(driver, namenode, 35321, None)\n",
      "  Registering block manager namenode:35321 with 366.3 MiB RAM, BlockManagerId(driver, namenode, 35321, None)\n",
      "  Registered BlockManager BlockManagerId(driver, namenode, 35321, None)\n",
      "  Initialized BlockManager: BlockManagerId(driver, namenode, 35321, None)\n",
      "  Started o.s.j.s.ServletContextHandler@1304e84b{/metrics/json,null,AVAILABLE,@Spark}\n",
      "  Block broadcast_0 stored as values in memory (estimated size 410.9 KiB, free 365.9 MiB)\n",
      "  Block broadcast_0_piece0 stored as bytes in memory (estimated size 42.1 KiB, free 365.9 MiB)\n",
      "  Added broadcast_0_piece0 in memory on namenode:35321 (size: 42.1 KiB, free: 366.3 MiB)\n",
      "  Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
      "  Total input files to process : 200\n",
      "  mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
      "  Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "  File Output Committer Algorithm version is 1\n",
      "  FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "  Starting job: runJob at SparkHadoopWriter.scala:83\n",
      "  Registering RDD 3 (combineByKey at /home/ubuntu/.local/lib/python3.8/site-packages/mrjob/spark/harness.py:490) as input to shuffle 0\n",
      "  Got job 0 (runJob at SparkHadoopWriter.scala:83) with 200 output partitions\n",
      "  Final stage: ResultStage 1 (runJob at SparkHadoopWriter.scala:83)\n",
      "  Parents of final stage: List(ShuffleMapStage 0)\n",
      "  Missing parents: List(ShuffleMapStage 0)\n",
      "  Submitting ShuffleMapStage 0 (PairwiseRDD[3] at combineByKey at /home/ubuntu/.local/lib/python3.8/site-packages/mrjob/spark/harness.py:490), which has no missing parents\n",
      "  Block broadcast_1 stored as values in memory (estimated size 51.7 KiB, free 365.8 MiB)\n",
      "  Block broadcast_1_piece0 stored as bytes in memory (estimated size 25.9 KiB, free 365.8 MiB)\n",
      "  Added broadcast_1_piece0 in memory on namenode:35321 (size: 25.9 KiB, free: 366.2 MiB)\n",
      "  Created broadcast 1 from broadcast at DAGScheduler.scala:1427\n",
      "  Submitting 200 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at combineByKey at /home/ubuntu/.local/lib/python3.8/site-packages/mrjob/spark/harness.py:490) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Adding task set 0.0 with 200 tasks resource profile 0\n",
      "  Starting task 0.0 in stage 0.0 (TID 0) (namenode, executor driver, partition 0, ANY, 4552 bytes) taskResourceAssignments Map()\n",
      "  Starting task 1.0 in stage 0.0 (TID 1) (namenode, executor driver, partition 1, ANY, 4552 bytes) taskResourceAssignments Map()\n",
      "  Starting task 2.0 in stage 0.0 (TID 2) (namenode, executor driver, partition 2, ANY, 4552 bytes) taskResourceAssignments Map()\n",
      "  Starting task 3.0 in stage 0.0 (TID 3) (namenode, executor driver, partition 3, ANY, 4552 bytes) taskResourceAssignments Map()\n",
      "  Running task 1.0 in stage 0.0 (TID 1)\n",
      "  Running task 0.0 in stage 0.0 (TID 0)\n",
      "  Running task 3.0 in stage 0.0 (TID 3)\n",
      "  Running task 2.0 in stage 0.0 (TID 2)\n",
      "  Input split: hdfs://namenode:9000/cleaned_data.csv/part-00001-fdd9117d-88f2-46ab-bb61-b6e2dde97885-c000.csv:0+28576357\n",
      "  Input split: hdfs://namenode:9000/cleaned_data.csv/part-00002-fdd9117d-88f2-46ab-bb61-b6e2dde97885-c000.csv:0+28518842\n",
      "  Input split: hdfs://namenode:9000/cleaned_data.csv/part-00003-fdd9117d-88f2-46ab-bb61-b6e2dde97885-c000.csv:0+28703975\n",
      "  Input split: hdfs://namenode:9000/cleaned_data.csv/part-00000-fdd9117d-88f2-46ab-bb61-b6e2dde97885-c000.csv:0+28568106\n",
      "  SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "  SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "  SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "  SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "  Times: total = 1141691, boot = 560, init = 982, finish = 1140149\n",
      "  Finished task 1.0 in stage 0.0 (TID 1). 1945 bytes result sent to driver\n",
      "  Starting task 4.0 in stage 0.0 (TID 4) (namenode, executor driver, partition 4, ANY, 4552 bytes) taskResourceAssignments Map()\n",
      "  Running task 4.0 in stage 0.0 (TID 4)\n",
      "  Input split: hdfs://namenode:9000/cleaned_data.csv/part-00004-fdd9117d-88f2-46ab-bb61-b6e2dde97885-c000.csv:0+28708654\n",
      "  Finished task 1.0 in stage 0.0 (TID 1) in 1143872 ms on namenode (executor driver) (1/200)\n",
      "  Connected to AccumulatorServer at host: 127.0.0.1 port: 57321\n",
      "  SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "  Times: total = 1144416, boot = 553, init = 920, finish = 1142943\n",
      "  Finished task 0.0 in stage 0.0 (TID 0). 1902 bytes result sent to driver\n",
      "  Starting task 5.0 in stage 0.0 (TID 5) (namenode, executor driver, partition 5, ANY, 4552 bytes) taskResourceAssignments Map()\n",
      "  Finished task 0.0 in stage 0.0 (TID 0) in 1145522 ms on namenode (executor driver) (2/200)\n",
      "  Running task 5.0 in stage 0.0 (TID 5)\n",
      "  Input split: hdfs://namenode:9000/cleaned_data.csv/part-00005-fdd9117d-88f2-46ab-bb61-b6e2dde97885-c000.csv:0+28742818\n",
      "  SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "  Times: total = 1144791, boot = 574, init = 876, finish = 1143341\n",
      "  Finished task 2.0 in stage 0.0 (TID 2). 1902 bytes result sent to driver\n",
      "  Starting task 6.0 in stage 0.0 (TID 6) (namenode, executor driver, partition 6, ANY, 4552 bytes) taskResourceAssignments Map()\n",
      "  Running task 6.0 in stage 0.0 (TID 6)\n",
      "  Finished task 2.0 in stage 0.0 (TID 2) in 1145760 ms on namenode (executor driver) (3/200)\n",
      "  Input split: hdfs://namenode:9000/cleaned_data.csv/part-00006-fdd9117d-88f2-46ab-bb61-b6e2dde97885-c000.csv:0+28617807\n",
      "  SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "  Times: total = 1150104, boot = 541, init = 975, finish = 1148588\n",
      "  Finished task 3.0 in stage 0.0 (TID 3). 1902 bytes result sent to driver\n",
      "  Starting task 7.0 in stage 0.0 (TID 7) (namenode, executor driver, partition 7, ANY, 4552 bytes) taskResourceAssignments Map()\n",
      "  Running task 7.0 in stage 0.0 (TID 7)\n",
      "  Input split: hdfs://namenode:9000/cleaned_data.csv/part-00007-fdd9117d-88f2-46ab-bb61-b6e2dde97885-c000.csv:0+28455597\n",
      "  Finished task 3.0 in stage 0.0 (TID 3) in 1150853 ms on namenode (executor driver) (4/200)\n",
      "  SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "  Times: total = 1126819, boot = -146, init = 184, finish = 1126781\n",
      "  Finished task 7.0 in stage 0.0 (TID 7). 1902 bytes result sent to driver\n",
      "  Starting task 8.0 in stage 0.0 (TID 8) (namenode, executor driver, partition 8, ANY, 4552 bytes) taskResourceAssignments Map()\n",
      "  Finished task 7.0 in stage 0.0 (TID 7) in 1127258 ms on namenode (executor driver) (5/200)\n",
      "  Running task 8.0 in stage 0.0 (TID 8)\n",
      "  Input split: hdfs://namenode:9000/cleaned_data.csv/part-00008-fdd9117d-88f2-46ab-bb61-b6e2dde97885-c000.csv:0+28736107\n",
      "  SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "  Times: total = 1139223, boot = -1679, init = 1735, finish = 1139167\n",
      "  Finished task 4.0 in stage 0.0 (TID 4). 1902 bytes result sent to driver\n",
      "  Starting task 9.0 in stage 0.0 (TID 9) (namenode, executor driver, partition 9, ANY, 4552 bytes) taskResourceAssignments Map()\n",
      "  Finished task 4.0 in stage 0.0 (TID 4) in 1139698 ms on namenode (executor driver) (6/200)\n",
      "  Running task 9.0 in stage 0.0 (TID 9)\n",
      "  Input split: hdfs://namenode:9000/cleaned_data.csv/part-00009-fdd9117d-88f2-46ab-bb61-b6e2dde97885-c000.csv:0+28661137\n",
      "  SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "  Times: total = 1142551, boot = -503, init = 602, finish = 1142452\n",
      "  Finished task 5.0 in stage 0.0 (TID 5). 1902 bytes result sent to driver\n",
      "  Starting task 10.0 in stage 0.0 (TID 10) (namenode, executor driver, partition 10, ANY, 4552 bytes) taskResourceAssignments Map()\n",
      "  Finished task 5.0 in stage 0.0 (TID 5) in 1142962 ms on namenode (executor driver) (7/200)\n",
      "  Running task 10.0 in stage 0.0 (TID 10)\n",
      "  Input split: hdfs://namenode:9000/cleaned_data.csv/part-00010-fdd9117d-88f2-46ab-bb61-b6e2dde97885-c000.csv:0+28656228\n",
      "  SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "  Times: total = 1153934, boot = -454, init = 521, finish = 1153867\n",
      "  Finished task 6.0 in stage 0.0 (TID 6). 1902 bytes result sent to driver\n",
      "  Starting task 11.0 in stage 0.0 (TID 11) (namenode, executor driver, partition 11, ANY, 4552 bytes) taskResourceAssignments Map()\n",
      "  Finished task 6.0 in stage 0.0 (TID 6) in 1154213 ms on namenode (executor driver) (8/200)\n",
      "  Running task 11.0 in stage 0.0 (TID 11)\n",
      "  Input split: hdfs://namenode:9000/cleaned_data.csv/part-00011-fdd9117d-88f2-46ab-bb61-b6e2dde97885-c000.csv:0+28564728\n",
      "  SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "  Times: total = 1181711, boot = -319, init = 373, finish = 1181657\n",
      "  Finished task 8.0 in stage 0.0 (TID 8). 1902 bytes result sent to driver\n",
      "  Starting task 12.0 in stage 0.0 (TID 12) (namenode, executor driver, partition 12, ANY, 4552 bytes) taskResourceAssignments Map()\n",
      "  Finished task 8.0 in stage 0.0 (TID 8) in 1182545 ms on namenode (executor driver) (9/200)\n",
      "  Running task 12.0 in stage 0.0 (TID 12)\n",
      "  Input split: hdfs://namenode:9000/cleaned_data.csv/part-00012-fdd9117d-88f2-46ab-bb61-b6e2dde97885-c000.csv:0+28571145\n",
      "  SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "  Times: total = 1183553, boot = -140, init = 204, finish = 1183489\n",
      "  Finished task 9.0 in stage 0.0 (TID 9). 1902 bytes result sent to driver\n",
      "  Starting task 13.0 in stage 0.0 (TID 13) (namenode, executor driver, partition 13, ANY, 4552 bytes) taskResourceAssignments Map()\n",
      "  Finished task 9.0 in stage 0.0 (TID 9) in 1183943 ms on namenode (executor driver) (10/200)\n",
      "  Running task 13.0 in stage 0.0 (TID 13)\n",
      "  Input split: hdfs://namenode:9000/cleaned_data.csv/part-00013-fdd9117d-88f2-46ab-bb61-b6e2dde97885-c000.csv:0+28770957\n",
      "  SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "  Times: total = 1188262, boot = -74, init = 132, finish = 1188204\n",
      "  Finished task 10.0 in stage 0.0 (TID 10). 1902 bytes result sent to driver\n",
      "  Starting task 14.0 in stage 0.0 (TID 14) (namenode, executor driver, partition 14, ANY, 4552 bytes) taskResourceAssignments Map()\n",
      "  Running task 14.0 in stage 0.0 (TID 14)\n",
      "  Finished task 10.0 in stage 0.0 (TID 10) in 1188510 ms on namenode (executor driver) (11/200)\n",
      "  Input split: hdfs://namenode:9000/cleaned_data.csv/part-00014-fdd9117d-88f2-46ab-bb61-b6e2dde97885-c000.csv:0+28759131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "  Times: total = 1178221, boot = -138, init = 177, finish = 1178182\n",
      "  Finished task 11.0 in stage 0.0 (TID 11). 1902 bytes result sent to driver\n",
      "  Starting task 15.0 in stage 0.0 (TID 15) (namenode, executor driver, partition 15, ANY, 4552 bytes) taskResourceAssignments Map()\n",
      "  Running task 15.0 in stage 0.0 (TID 15)\n",
      "  Finished task 11.0 in stage 0.0 (TID 11) in 1178489 ms on namenode (executor driver) (12/200)\n",
      "  Input split: hdfs://namenode:9000/cleaned_data.csv/part-00015-fdd9117d-88f2-46ab-bb61-b6e2dde97885-c000.csv:0+28631978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-29 14:00:59,026 ERROR client.TransportResponseHandler: Still have 1 requests outstanding when connection from /10.10.22.91:59354 is closed\n",
      "2022-04-29 14:00:59,027 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to get executor loss reason for executor id 4 at RPC address 10.10.22.91:59364, but got no response. Marking as agent lost.\n",
      "java.io.IOException: Connection from /10.10.22.91:59354 closed\n",
      "\tat org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:147)\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelInactive(TransportChannelHandler.java:117)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)\n",
      "\tat io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)\n",
      "\tat io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:277)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)\n",
      "\tat io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)\n",
      "\tat org.apache.spark.network.util.TransportFrameDecoder.channelInactive(TransportFrameDecoder.java:225)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)\n",
      "\tat io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:831)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:497)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "2022-04-29 14:00:59,029 ERROR cluster.YarnScheduler: Lost executor 4 on datanode3: Executor Process Lost\n",
      "2022-04-29 14:00:59,312 ERROR client.TransportClient: Failed to send RPC RPC 4949351371267258696 to /10.10.22.91:59354: io.netty.channel.StacklessClosedChannelException\n",
      "io.netty.channel.StacklessClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source)\n",
      "2022-04-29 14:00:59,313 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to get executor loss reason for executor id 3 at RPC address 10.10.22.210:56984, but got no response. Marking as agent lost.\n",
      "java.io.IOException: Failed to send RPC RPC 4949351371267258696 to /10.10.22.91:59354: io.netty.channel.StacklessClosedChannelException\n",
      "\tat org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:392)\n",
      "\tat org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:369)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:552)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:491)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:616)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:609)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:1017)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:878)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1071)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: io.netty.channel.StacklessClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source)\n",
      "2022-04-29 14:00:59,313 ERROR cluster.YarnScheduler: Lost executor 3 on datanode1: Executor Process Lost\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/ubuntu/mapred.py\", line 42, in <module>\r\n",
      "    subreddit.run()\r\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/mrjob/job.py\", line 616, in run\r\n",
      "    cls().execute()\r\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/mrjob/job.py\", line 687, in execute\r\n",
      "    self.run_job()\r\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/mrjob/job.py\", line 636, in run_job\r\n",
      "    runner.run()\r\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/mrjob/runner.py\", line 503, in run\r\n",
      "    self._run()\r\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/mrjob/spark/runner.py\", line 189, in _run\r\n",
      "    self._run_steps_on_spark()\r\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/mrjob/spark/runner.py\", line 294, in _run_steps_on_spark\r\n",
      "    self._run_step_on_spark(group['steps'][0], step_num, last_step_num)\r\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/mrjob/spark/runner.py\", line 327, in _run_step_on_spark\r\n",
      "    returncode, step_interpretation = self._run_spark_submit(\r\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/mrjob/bin.py\", line 863, in _run_spark_submit\r\n",
      "    _parse_spark_log(\r\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/mrjob/logs/spark.py\", line 38, in _parse_spark_log\r\n"
     ]
    }
   ],
   "source": [
    "!python3 /home/ubuntu/mapred.py -r spark hdfs://namenode:9000/cleaned_data.csv >english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group_by_day =df2.groupBy(\"created_utc\",\"subreddit\",\"parent_id\").agg(f.sum(\"ups\").alias(\"total_ups\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group_by_day_body =df2.groupBy(\"created_utc\",\"subreddit\",\"parent_id\").agg(f.sum(\"ups\").alias(\"total_ups\"),f.collect_list(\"body\").alias(\"body\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group_by_day_body.filter(df_group_by_day_body.subreddit == \"AskReddit\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline('summarization', model='facebook/bart-large', tokenizer='facebook/bart-large-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_top_comments = df_top_comments.persist(ps.StorageLevel.DISK_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only English Columns\n",
    "# df2 = df2.filter(df2.is_english == \"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = df2.withColumn(\"is_english\",isEnglishUDF(f.col(\"body\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all link_ids for top comments\n",
    "# top_comments_link_ids=df_top_comments.rdd.map(lambda x: x.link_id).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_comments_link_ids =list(set(top_comments_link_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(top_comments_link_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import dayofmonth, mean, countDistinct, to_date, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_english = df_english.withColumn(\"day_of_month\", dayofmonth(\"created_utc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_group_by_day.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sparknlp.pretrained import PretrainedPipeline, LanguageDetectorDL, PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_group_by_day.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group_by_day.where(df_group_by_day.subreddit=='AskReddit').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_top_comments.select('English').show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment_section = ['you just know someone has made that joke with her',\n",
    "#  'was literally discussing this with my wife yesterday i am british she is not for some reason she thought i would know the answer to whether that would be a thing or not i did not so i guess it s a pretty common shower thought tbh',\n",
    "#  'i m giving you mine as a thank you for telling us this']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \".\".join(comment_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarizer = pipeline('summarization', model='facebook/bart-large-cnn', tokenizer='facebook/bart-large-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarizer(text, min_length = round(0.1 * len(text.split(' '))), max_length = round(0.2 * len(text.split(' '))), do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandasDF = df_group_by_day.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandasDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandasDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandasDF.to_csv(r'pandas.txt', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
